{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Comparison\n",
    "================\n",
    "\n",
    "Common questions when fitting a model to data are: what model should I use? How many parameters should the model have?\n",
    "Is the model too complex or too simple?\n",
    "\n",
    "Model comparison provides answers to these questions. It amounts to composing and fitting many different models to the data\n",
    "and comparing how well they fit.\n",
    "\n",
    "This example illustrates model comparison using the noisy 1D signal example. We fit a dataset consisting of two\n",
    "Gaussians and fit it with three models comprised of 1, 2 and 3 Gaussian's respectively.\n",
    "\n",
    "Using the Bayesian evidence to compare the models, we favour the model with 2 Gaussians, which is the \"correct\" model\n",
    "given that it was the model used to simulate the dataset in the first place.\n",
    "\n",
    "__Metrics__\n",
    "\n",
    "Different metrics can be used compare models and quantify their `goodness-of-fit`.\n",
    "\n",
    "In this example we show the results of using two different metrics:\n",
    "\n",
    " - `log_likelihood`: The value returned by the `log_likelihood_function` of an `Analysis` object. which is directly\n",
    "   related to the sum of the residuals squared (e.g. the `chi_squared`). The log likelihood does not change when more\n",
    "   or less parameters are included in the model, therefore it does not account for over-fitting and will often favour\n",
    "   more complex models irrespective of whether they fit the data better.\n",
    "\n",
    " - `log_evidence`: The Bayesian evidence, which is closely related to the log likelihood but utilizes additional\n",
    "   information which penalizes models based on their complexity. The Bayesian evidence will therefore favour simpler\n",
    "   models over more complex models, unless the more complex model provides a much better fit to the data.\n",
    "\n",
    "__Example Source Code (`af.ex`)__\n",
    "\n",
    "The **PyAutoFit** source code has the following example objects (accessed via `af.ex`) used in this tutorial:\n",
    "\n",
    " - `Analysis`: an analysis object which fits noisy 1D datasets, including `log_likelihood_function` and\n",
    " `visualize` functions.\n",
    "\n",
    " - `Gaussian`: a model component representing a 1D Gaussian profile.\n",
    "\n",
    "These are functionally identical to the `Analysis` and `Gaussian` objects you have seen previously."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "%matplotlib inline\n",
    "from pyprojroot import here\n",
    "workspace_path = str(here())\n",
    "%cd $workspace_path\n",
    "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "import autofit as af"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data__\n",
    "\n",
    "Load data of two 1D Gaussians from a .json file in the directory `autofit_workspace/dataset/gaussian_x2`.\n",
    "\n",
    "This 1D data was created using two 1D Gaussians, therefore model comparison should favor a model with two Gaussians over \n",
    "a models with 1 or 3 Gaussians."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x2\")\n",
    "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
    "noise_map = af.util.numpy_array_from_json(\n",
    "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "xvalues = range(data.shape[0])\n",
    "\n",
    "plt.errorbar(\n",
    "    x=xvalues, y=data, yerr=noise_map, color=\"k\", ecolor=\"k\", linestyle=\" \", elinewidth=1, capsize=2\n",
    ")\n",
    "plt.title(\"1D Gaussian Dataset Used For Model Comparison.\")\n",
    "plt.xlabel(\"x values of profile\")\n",
    "plt.ylabel(\"Profile normalization\")\n",
    "plt.show()\n",
    "plt.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Model x1 Gaussian__\n",
    "\n",
    "Create a model to fit the data, starting with a model where the data is fitted with 1 Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "model = af.Collection(gaussian_0=af.ex.Gaussian)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `info` attribute shows the model in a readable format, showing it contains one `Gaussian`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "print(model.info)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the analysis which fits the model to the data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "analysis = af.ex.Analysis(data=data, noise_map=noise_map)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the data using a non-linear search, to determine the goodness of fit of this model.\n",
    "\n",
    "We use the nested sampling algorithm Dynesty, noting that the Bayesian evidence (`log_evidence`) of a model can only\n",
    "be estimated using a nested sampling algorithm."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "search = af.DynestyStatic(\n",
    "    path_prefix=path.join(\"session_3\"),\n",
    "    name=\"gaussian_x1\",\n",
    "    nlive=50,\n",
    "    sample=\"rwalk\",\n",
    "    iterations_per_update=3000\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the fit."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "result_x1_gaussian = search.fit(model=model, analysis=analysis)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are concisely summarised using the `result.info` property.\n",
    "\n",
    "These show that the parameters of the Gaussian are well constrained, with small errors on their inferred values.\n",
    "However, it does not inform us of whether the model provides a good fit to the data overall."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "print(result_x1_gaussian.info)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum log likelihood model is used to visualize the fit.\n",
    "\n",
    "For 1 Gaussian, residuals are visible, whereby the model Gaussian cannot fit the highest central data-point and \n",
    "there is a mismatch at the edges of the profile around pixels 40 and 60.\n",
    "\n",
    "Based on visual inspection, the model therefore provides a poor fit to the data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "instance = result_x1_gaussian.max_log_likelihood_instance\n",
    "\n",
    "gaussian_0 = instance.gaussian_0.model_data_1d_via_xvalues_from(\n",
    "    xvalues=np.arange(data.shape[0])\n",
    ")\n",
    "model_data = gaussian_0\n",
    "\n",
    "plt.errorbar(\n",
    "    x=xvalues, y=data, yerr=noise_map, color=\"k\", ecolor=\"k\", linestyle=\" \", elinewidth=1, capsize=2\n",
    ")\n",
    "plt.plot(range(data.shape[0]), model_data, color=\"r\")\n",
    "plt.plot(range(data.shape[0]), gaussian_0, \"--\")\n",
    "plt.title(\"Model fit using 1 Gaussian.\")\n",
    "plt.xlabel(\"x values of profile\")\n",
    "plt.ylabel(\"Profile normalization\")\n",
    "plt.show()\n",
    "plt.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the `log_likelihood` and `log_evidence` of this model-fit, which we will compare to more complex models in order \n",
    "to determine which model provides the best fit to the data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "print(\"1 Gaussian:\")\n",
    "print(f\"Log Likelihood: {max(result_x1_gaussian.samples.log_likelihood_list)}\")\n",
    "print(f\"Log Evidence: {result_x1_gaussian.samples.log_evidence}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Model x2 Gaussian__\n",
    "\n",
    "We now create a model to fit the data which consists of 2 Gaussians."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "model = af.Collection(gaussian_0=af.ex.Gaussian, gaussian_1=af.ex.Gaussian)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `info` attribute shows the model now consists of two `Gaussian`'s."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "print(model.info)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the steps above to create the non-linear search and perform the model-fit."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "search = af.DynestyStatic(\n",
    "    path_prefix=path.join(\"session_3\"),\n",
    "    name=\"gaussian_x2\",\n",
    "    nlive=50,\n",
    "    sample=\"rwalk\",\n",
    "    iterations_per_update=3000\n",
    ")\n",
    "\n",
    "result_x2_gaussian = search.fit(model=model, analysis=analysis)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that two Gaussians have now been fitted to the data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "print(result_x2_gaussian.info)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the fit, we see that the problems with the previous fit have been addressed. The central data-point at the \n",
    "highest normalization is fitted correctly and the residuals at the edges of the profile around pixels 40 and 60 are \n",
    "significantly reduced.\n",
    "\n",
    "There are effectively no residuals, indicating that the model provides a good fit to the data.\n",
    "\n",
    "The residuals are so small that they are consistent with noise in the data. One therefore should not expect that \n",
    "a more complex model than one with 2 Gaussians can provide a better fit."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "instance = result_x2_gaussian.max_log_likelihood_instance\n",
    "\n",
    "gaussian_0 = instance.gaussian_0.model_data_1d_via_xvalues_from(\n",
    "    xvalues=np.arange(data.shape[0])\n",
    ")\n",
    "gaussian_1 = instance.gaussian_1.model_data_1d_via_xvalues_from(\n",
    "    xvalues=np.arange(data.shape[0])\n",
    ")\n",
    "model_data = gaussian_0 + gaussian_1\n",
    "\n",
    "plt.errorbar(\n",
    "    x=xvalues, y=data, yerr=noise_map, color=\"k\", ecolor=\"k\", linestyle=\" \", elinewidth=1, capsize=2\n",
    ")\n",
    "plt.plot(range(data.shape[0]), model_data, color=\"r\")\n",
    "plt.plot(range(data.shape[0]), gaussian_0, \"--\")\n",
    "plt.plot(range(data.shape[0]), gaussian_1, \"--\")\n",
    "plt.title(\"Model fit using 2 Gaussian.\")\n",
    "plt.xlabel(\"x values of profile\")\n",
    "plt.ylabel(\"Profile normalization\")\n",
    "plt.show()\n",
    "plt.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the `log_likelihood` and `log_evidence` of this model-fit, and compare these values to the previous model-fit\n",
    "which used 1 Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "print(\"1 Gaussian:\")\n",
    "print(f\"Log Likelihood: {max(result_x1_gaussian.samples.log_likelihood_list)}\")\n",
    "print(f\"Log Evidence: {result_x1_gaussian.samples.log_evidence}\")\n",
    "\n",
    "print(\"2 Gaussians:\")\n",
    "print(f\"Log Likelihood: {max(result_x2_gaussian.samples.log_likelihood_list)}\")\n",
    "print(f\"Log Evidence: {result_x2_gaussian.samples.log_evidence}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the `log_likelihood` and `log_evidence` have increased significantly, indicating that the model with 2 Gaussians\n",
    "is favored over the model with 1 Gaussian.\n",
    "\n",
    "This is expected, as we know the data was generated using 2 Gaussians!\n",
    "\n",
    "__Model x3 Gaussian__\n",
    "\n",
    "We now create a model to fit the data which consists of 3 Gaussians."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "model = af.Collection(gaussian_0=af.ex.Gaussian, gaussian_1=af.ex.Gaussian, gaussian_2=af.ex.Gaussian)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `info` attribute shows the model consists of three `Gaussian`'s."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "print(model.info)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the steps above to create the non-linear search and perform the model-fit."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "search = af.DynestyStatic(\n",
    "    path_prefix=path.join(\"session_3\"),\n",
    "    name=\"gaussian_x3\",\n",
    "    nlive=50,\n",
    "    sample=\"rwalk\",\n",
    "    iterations_per_update=3000\n",
    ")\n",
    "\n",
    "result_x3_gaussian = search.fit(model=model, analysis=analysis)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that three Gaussians have now been fitted to the data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "print(result_x3_gaussian.info)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the fit, we see that there are effectively no residuals, indicating that the model provides a good fit.\n",
    "\n",
    "By eye, this fit looks as good as the 2 Gaussian model above."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "instance = result_x3_gaussian.max_log_likelihood_instance\n",
    "\n",
    "gaussian_0 = instance.gaussian_0.model_data_1d_via_xvalues_from(\n",
    "    xvalues=np.arange(data.shape[0])\n",
    ")\n",
    "gaussian_1 = instance.gaussian_1.model_data_1d_via_xvalues_from(\n",
    "    xvalues=np.arange(data.shape[0])\n",
    ")\n",
    "gaussian_2 = instance.gaussian_2.model_data_1d_via_xvalues_from(\n",
    "    xvalues=np.arange(data.shape[0])\n",
    ")\n",
    "model_data = gaussian_0 + gaussian_1 + gaussian_2\n",
    "\n",
    "plt.errorbar(\n",
    "    x=xvalues, y=data, yerr=noise_map, color=\"k\", ecolor=\"k\", linestyle=\" \", elinewidth=1, capsize=2\n",
    ")\n",
    "plt.plot(range(data.shape[0]), model_data, color=\"r\")\n",
    "plt.plot(range(data.shape[0]), gaussian_0, \"--\")\n",
    "plt.plot(range(data.shape[0]), gaussian_1, \"--\")\n",
    "plt.plot(range(data.shape[0]), gaussian_2, \"--\")\n",
    "plt.title(\"Model fit using 3 Gaussian.\")\n",
    "plt.xlabel(\"x values of profile\")\n",
    "plt.ylabel(\"Profile normalization\")\n",
    "plt.show()\n",
    "plt.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the `log_likelihood` and `log_evidence` of this model-fit, and compare these values to the previous model-fit\n",
    "which used 1 and 2 Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "print(\"1 Gaussian:\")\n",
    "print(f\"Log Likelihood: {max(result_x1_gaussian.samples.log_likelihood_list)}\")\n",
    "print(f\"Log Evidence: {result_x1_gaussian.samples.log_evidence}\")\n",
    "\n",
    "print(\"2 Gaussians:\")\n",
    "print(f\"Log Likelihood: {max(result_x2_gaussian.samples.log_likelihood_list)}\")\n",
    "print(f\"Log Evidence: {result_x2_gaussian.samples.log_evidence}\")\n",
    "\n",
    "print(\"3 Gaussians:\")\n",
    "print(f\"Log Likelihood: {max(result_x3_gaussian.samples.log_likelihood_list)}\")\n",
    "print(f\"Log Evidence: {result_x3_gaussian.samples.log_evidence}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see an interesting result. The `log_likelihood` of the 3 Gaussian model is higher than the 2 Gaussian model\n",
    "(albeit, only slightly higher). However, the `log_evidence` is lower than the 2 Gaussian model.\n",
    "\n",
    "This confirms the behavior discussed at the start of the tutorial. The Bayesian evidence penalizes models with more \n",
    "freedom to fit the data, unless they provide a significantly better fit to the data. Using the evidence we favor the\n",
    "2 Gaussian model over the 3 Gaussian model for this reason, whereas using the likelihood we favor the 3 Gaussian model.\n",
    "\n",
    "__Wrap Up__\n",
    "\n",
    "We have shown how one can perform Bayesian model comparison, in order to determine which model provides the best-fit\n",
    "to the data. We saw how the Bayesian evidence penalizes models with more freedom to fit the data, unless they provide\n",
    "a significantly better fit to the data.\n",
    "\n",
    "For your research problem, now consider if there are multiple models that you could use to fit your data. Do you know\n",
    "which will provide the best-fit? Or will you need to use model comparison to determine this? Will certain models work\n",
    "on datasets of a certain quality, but simpler models be required for lower quality datasets?\n",
    "\n",
    "Multiple Datasets\n",
    "=================\n",
    "\n",
    "It is common to have multiple observations of the same signal. For the 1D Gaussian example, this would be multiple\n",
    "1D datasets of the same underlying Gaussian, with different noise-map realizations. In this situation, fitting the\n",
    "same model to all datasets simultaneously is desired, and would provide better constraints on the model.\n",
    "\n",
    "On other occations, the signal may vary across the datasets in a way that requires that the model is updated\n",
    "accordingly. For example, a scenario where the centre of each Gaussian is the same across the datasets, but\n",
    "their `sigma` values are different in each dataset. A model where all Gaussians share the same `centre` is now required.\n",
    "\n",
    "This examples illustrates how to perform model-fits to multiple datasets simultaneously, including tools to customize\n",
    "the model composition such that specific parameters of the model vary across the datasets.\n",
    "\n",
    "This uses the summing of `Analysis` object, which each have their own unique dataset and `log_likelihood_function`.\n",
    "Unique `Analysis` objects can be written for each dataset, meaning that we can perform model-fits to diverse datasets\n",
    "with different formats and structures.\n",
    "\n",
    "It is also common for each individual dataset to only constrain specific aspects of a model. The high level of model\n",
    "customizaiton ensures that composing a model that is appropriate for fitting to such large datasets is straight\n",
    "forward.\n",
    "\n",
    "__Example Source Code (`af.ex`)__\n",
    "\n",
    "The **PyAutoFit** source code has the following example objects (accessed via `af.ex`) used in this tutorial:\n",
    "\n",
    " - `Analysis`: an analysis object which fits noisy 1D datasets, including `log_likelihood_function` and \n",
    " `visualize` functions.\n",
    "\n",
    " - `Gaussian`: a model component representing a 1D Gaussian profile.\n",
    "\n",
    "These are functionally identical to the `Analysis` and `Gaussian` objects you have seen elsewhere in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "%matplotlib inline\n",
    "from pyprojroot import here\n",
    "workspace_path = str(here())\n",
    "%cd $workspace_path\n",
    "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "import autofit as af\n",
    "import autofit.plot as aplt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data__\n",
    "\n",
    "First, lets load 3 datasets of a 1D Gaussian, by loading them from .json files in the directory \n",
    "`autofit_workspace/dataset/`.\n",
    "\n",
    "All three datasets contain an identical signal, meaning that it is appropriate to fit the same model to all three \n",
    "datasets simultaneously.\n",
    "\n",
    "Each dataset has a different noise realization, meaning that performing a simultaneously fit will offer improved \n",
    "constraints over individual fits."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "dataset_size = 3\n",
    "\n",
    "data_list = []\n",
    "noise_map_list = []\n",
    "\n",
    "for dataset_index in range(dataset_size):\n",
    "    dataset_path = path.join(\n",
    "        \"dataset\", \"example_1d\", f\"gaussian_x1_identical_{dataset_index}\"\n",
    "    )\n",
    "\n",
    "    data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
    "    data_list.append(data)\n",
    "\n",
    "    noise_map = af.util.numpy_array_from_json(\n",
    "        file_path=path.join(dataset_path, \"noise_map.json\")\n",
    "    )\n",
    "    noise_map_list.append(noise_map)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets plot all 3 datasets, including their error bars. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "for data, noise_map in zip(data_list, noise_map_list):\n",
    "    xvalues = range(data.shape[0])\n",
    "\n",
    "    plt.errorbar(\n",
    "        x=xvalues,\n",
    "        y=data,\n",
    "        yerr=noise_map,\n",
    "        color=\"k\",\n",
    "        ecolor=\"k\",\n",
    "        linestyle=\" \",\n",
    "        elinewidth=1,\n",
    "        capsize=2,\n",
    "    )\n",
    "    plt.show()\n",
    "    plt.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Model__\n",
    "\n",
    "Next, we create our model, which corresponds to a single 1D Gaussian, that is fitted to all 3 datasets simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "model = af.Model(af.ex.Gaussian)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkout `autofit_workspace/config/priors/model.yaml`, this config file defines the default priors of the `Gaussian` \n",
    "model component. \n",
    "\n",
    "We overwrite the priors below to make them explicit."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "model.centre = af.UniformPrior(lower_limit=0.0, upper_limit=100.0)\n",
    "model.normalization = af.LogUniformPrior(lower_limit=1e-2, upper_limit=1e2)\n",
    "model.sigma = af.GaussianPrior(\n",
    "    mean=10.0, sigma=5.0, lower_limit=0.0, upper_limit=np.inf\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Analysis__\n",
    "\n",
    "We set up our three instances of the `Analysis` class, which is the same as the one we wrote in a previous session.\n",
    "\n",
    "We set up an `Analysis` for each dataset one-by-one, using a for loop:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "analysis_list = []\n",
    "\n",
    "for data, noise_map in zip(data_list, noise_map_list):\n",
    "    analysis = af.ex.Analysis(data=data, noise_map=noise_map)\n",
    "    analysis_list.append(analysis)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Analysis Summing__\n",
    "\n",
    "We now sum together every analysis in the list, to produce an overall analysis class which we fit with the non-linear\n",
    "search.\n",
    "\n",
    "By summing analysis objects the following happen:\n",
    "\n",
    " - The log likelihood values computed by the `log_likelihood_function` of each individual analysis class are summed to\n",
    "   give an overall log likelihood value that the non-linear search uses for model-fitting.\n",
    "\n",
    " - The output path structure of the results goes to a single folder, which includes sub-folders for the visualization\n",
    "   of every individual analysis object based on the `Analysis` object's `visualize` method."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "analysis = analysis_list[0] + analysis_list[1] + analysis_list[2]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can alternatively sum a list of analysis objects as follows:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "analysis = sum(analysis_list)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `log_likelihood_function`'s can be called in parallel over multiple cores by changing the `n_cores` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "analysis.n_cores = 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Search__\n",
    "\n",
    "To fit multiple datasets via a non-linear search we use this summed analysis object:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "search = af.DynestyStatic(\n",
    "    path_prefix=path.join(\"session_3\"),\n",
    "    name=\"multiple_datasets_simple\",\n",
    "    sample=\"rwalk\",\n",
    "    iterations_per_update=3000\n",
    ")\n",
    "\n",
    "result_list = search.fit(model=model, analysis=analysis)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Result__\n",
    "\n",
    "The result object returned by the fit is a list of the `Result` objects you have used in other examples.\n",
    "\n",
    "In this example, the same model is fitted across all analyses, thus every `Result` in the `result_list` contains\n",
    "the same information on the samples and thus gives the same output from methods such \n",
    "as `max_log_likelihood_instance`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "print(result_list[0].max_log_likelihood_instance.sigma)\n",
    "print(result_list[1].max_log_likelihood_instance.sigma)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the model-fit to each dataset by iterating over the results:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "for data, result in zip(data_list, result_list):\n",
    "    instance = result.max_log_likelihood_instance\n",
    "\n",
    "    model_data = instance.model_data_1d_via_xvalues_from(\n",
    "        xvalues=np.arange(data.shape[0])\n",
    "    )\n",
    "\n",
    "    plt.errorbar(\n",
    "        x=xvalues,\n",
    "        y=data,\n",
    "        yerr=noise_map,\n",
    "        color=\"k\",\n",
    "        ecolor=\"k\",\n",
    "        elinewidth=1,\n",
    "        capsize=2,\n",
    "    )\n",
    "    plt.plot(xvalues, model_data, color=\"r\")\n",
    "    plt.title(\"Dynesty model fit to 1D Gaussian dataset.\")\n",
    "    plt.xlabel(\"x values of profile\")\n",
    "    plt.ylabel(\"Profile normalization\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Variable Model Across Datasets__\n",
    "\n",
    "Above, the same model was fitted to every dataset simultaneously, which was possible because all 3 datasets contained \n",
    "an identical signal with only the noise varying across the datasets.\n",
    "\n",
    "It is common for the signal in each dataset to be different and for it to constrain only certain aspects of the model.\n",
    "The model parameterization therefore needs to change in order to account for this.\n",
    "\n",
    "Lets look at an example of a dataset of 3 1D Gaussians where the signal varies across the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x1_variable\")\n",
    "\n",
    "dataset_name_list = [\"sigma_0\", \"sigma_1\", \"sigma_2\"]\n",
    "\n",
    "data_list = []\n",
    "noise_map_list = []\n",
    "\n",
    "for dataset_name in dataset_name_list:\n",
    "    dataset_time_path = path.join(dataset_path, dataset_name)\n",
    "\n",
    "    data = af.util.numpy_array_from_json(\n",
    "        file_path=path.join(dataset_time_path, \"data.json\")\n",
    "    )\n",
    "    noise_map = af.util.numpy_array_from_json(\n",
    "        file_path=path.join(dataset_time_path, \"noise_map.json\")\n",
    "    )\n",
    "\n",
    "    data_list.append(data)\n",
    "    noise_map_list.append(noise_map)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plot these datasets, we see that the `sigma` of each Gaussian decreases.\n",
    "\n",
    "We will illustrate models which vary over the data based on this `sigma` value. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "for data, noise_map in zip(data_list, noise_map_list):\n",
    "    xvalues = range(data.shape[0])\n",
    "\n",
    "    af.ex.plot_profile_1d(xvalues=xvalues, profile_1d=data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the `centre` and `normalization` of all three 1D Gaussians are the same in each dataset,\n",
    "but their `sigma` values are decreasing.\n",
    "\n",
    "We therefore wish to compose and to fit a model to all three datasets simultaneously, where the `centre` \n",
    "and `normalization` are the same across all three datasets but the `sigma` value is unique for each dataset.\n",
    "\n",
    "To do that, we interface a model with a summed list of analysis objects, which we create below for this new\n",
    "dataset:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "analysis_list = []\n",
    "\n",
    "for data, noise_map in zip(data_list, noise_map_list):\n",
    "    analysis = af.ex.Analysis(data=data, noise_map=noise_map)\n",
    "    analysis_list.append(analysis)\n",
    "\n",
    "analysis = sum(analysis_list)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next compose a model of a 1D Gaussian, as performed frequently throughout the **PyAutoFit** examples:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "model = af.Collection(gaussian=af.Model(af.ex.Gaussian))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now update the model using the summed `Analysis `objects to compose a model where: \n",
    "\n",
    " - The `centre` and `normalization` values of the Gaussian fitted to every dataset in every `Analysis` object are\n",
    " identical. \n",
    "\n",
    " - The `sigma` value of the every Gaussian fitted to every dataset in every `Analysis` object are different.\n",
    "\n",
    "This means that the model has 5 free parameters in total, the shared `centre` and `normalization` and a unique\n",
    "`sigma` value for every dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "analysis = analysis.with_free_parameters(model.gaussian.sigma)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To inspect this new model, with extra parameters for each dataset created, we have to extract the modified\n",
    "version of this model from the `Analysis` object.\n",
    "\n",
    "This occurs automatically when we begin a non-linear search, therefore the normal `model` we created above\n",
    "is what we input to the `search.fit()` method."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_updated = analysis.modify_model(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the model has 5 free parameters in total, the shared `centre` and `normalization` and a unique\n",
    "`sigma` value for every dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "print(model_updated.total_free_parameters)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now fit this model to the data using the usual **PyAutoFit** tools:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "search = af.DynestyStatic(\n",
    "    path_prefix=path.join(\"session_3\"),\n",
    "    name=\"multiple_datasets_free_sigma\",\n",
    "    sample=\"rwalk\"\n",
    ")\n",
    "\n",
    "result_list = search.fit(model=model, analysis=analysis)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Variable Parameters As Relationship__\n",
    "\n",
    "In the model above, an extra free parameter `sigma` was added for every dataset. \n",
    "\n",
    "This was ok for the simple model fitted here, to just 3 datasets. However, for more complex models and problems\n",
    "with 10+ datasets one will quickly find that the model complexity increases dramatically.\n",
    "\n",
    "In these circumstances, one can instead compose a model where the parameters vary smoothly across the datasets\n",
    "via a user defined relation.\n",
    "\n",
    "Below, we compose a model where the `sigma` value fitted to each dataset is computed according to:\n",
    "\n",
    " `y = m * x + c` : `sigma` = sigma_m * x + sigma_c`\n",
    "\n",
    "Where x is an integer number specifying the index of the dataset (e.g. 1, 2 and 3).\n",
    "\n",
    "By defining a relation of this form, `sigma_m` and `sigma_c` are the only free parameters of the model which vary\n",
    "across the datasets. \n",
    "\n",
    "Therefore, if more datasets are added the number of model parameter does not increase, like we saw above."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "sigma_m = af.UniformPrior(lower_limit=-10.0, upper_limit=10.0)\n",
    "sigma_c = af.UniformPrior(lower_limit=-10.0, upper_limit=10.0)\n",
    "\n",
    "x_list = [1.0, 2.0, 3.0]\n",
    "\n",
    "analysis_with_relation_list = []\n",
    "\n",
    "for x, analysis in zip(x_list, analysis_list):\n",
    "    sigma_relation = (sigma_m * x) + sigma_c\n",
    "\n",
    "    analysis_with_relation = analysis.with_model(\n",
    "        model.replacing({model.gaussian.sigma: sigma_relation})\n",
    "    )\n",
    "\n",
    "    analysis_with_relation_list.append(analysis_with_relation)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit this model as per usual, you may wish to checkout the `model.info` file to see how a schematic of this\n",
    "model's composition."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "analysis_with_relation = sum(analysis_with_relation_list)\n",
    "\n",
    "search = af.DynestyStatic(\n",
    "    path_prefix=path.join(\"session_3\"),\n",
    "    name=\"multiple_datasets_relation\",\n",
    "    sample=\"rwalk\"\n",
    ")\n",
    "\n",
    "result_list = search.fit(model=model, analysis=analysis_with_relation)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Temporally Varying Models__\n",
    "\n",
    "An obvious example of fitting models which vary across datasets are time-varying models, where the datasets are\n",
    "observations of a signal which varies across time.\n",
    "\n",
    "In such circumstances, it is common for certain model parameters to be known to not vary as a function of time (and \n",
    "therefore be fixed across the datasets) whereas other parameters are known to vary as a function of time (and therefore\n",
    "should be parameterized accordingly using the API illustrated here).\n",
    "\n",
    "__Different Analysis Objects__\n",
    "\n",
    "For simplicity, this example summed together only a single `Analysis` class. \n",
    "\n",
    "For many problems one may have multiple datasets which are quite different in their format and structure (perhaps \n",
    "one is a  1D signal whereas another dataset is an image). In this situation, one can simply define unique `Analysis`\n",
    "objects for each type of dataset, which will contain a unique `log_likelihood_function` and methods for visualization.\n",
    "\n",
    "Nevertheless, the analysis summing API illustrated here will still work, meaning that **PyAutoFit** makes it simple to \n",
    "fit highly customized models to multiple datasets that are different in their format and structure. \n",
    "\n",
    "__Wrap Up__\n",
    "\n",
    "We have shown how to fit large datasets simultaneously, using custom models that vary specific\n",
    "parameters across the dataset.\n",
    "\n",
    "Now think about your specific research problem, and whether you may find yourself in a situation where you wish to\n",
    "fit a model to multiple datasets simultaneously. If so, how might the model need to be composed to achieve this?\n",
    "Will parameters be shared across datasets, or will they vary? Will the datasets be of the same format and structure,\n",
    "or different?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
